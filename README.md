# å°èªèªéŸ³è¾¨è­˜ç³»çµ±ï¼ˆTaiwanese Hokkien Speech-to-Textï¼‰

æœ¬å°ˆæ¡ˆæ—¨åœ¨æä¾›ä¸€å¥—å®Œæ•´ä¸”å¯æ“´å±•çš„å°èªï¼ˆè‡ºç£é–©å—èªï¼‰èªéŸ³è¾¨è­˜è§£æ±ºæ–¹æ¡ˆï¼Œæ¶µè“‹å¾èªéŸ³ âœ æ‹¼éŸ³ âœ æ¼¢å­—çš„é›™éšæ®µæ¶æ§‹ï¼Œä»¥åŠåŸºæ–¼ LoRA å¾®èª¿çš„ Whisper æ¨¡å‹ï¼Œä¸¦åŒæ™‚æ”¯æ´æœ¬åœ°èˆ‡é›²ç«¯éƒ¨ç½²ã€‚æ•´é«”æŠ€è¡“æ£§æ¡ç”¨ PyTorchã€Hugging Face Transformersã€PEFTã€Accelerate ç­‰å…ˆé€²å·¥å…·ï¼Œç¢ºä¿è¨“ç·´æ•ˆèƒ½ã€æ¨è«–æ•ˆç‡èˆ‡æ˜“ç”¨æ€§ã€‚

ğŸ”— **ç·šä¸Šé«”é©—**ï¼š[Hugging Face Spaces - KikKoh/Hokkien](https://huggingface.co/spaces/KikKoh/Hokkien)

---

## ğŸ¯ å°ˆæ¡ˆäº®é»

* **é›™éšæ®µæ¶æ§‹**ï¼š

  * **Stage 1**ï¼šå°èªèªéŸ³ âœ ç¾…é¦¬æ‹¼éŸ³ï¼ˆå°ç¾…ï¼‰ (my-wav2vec2 æ¨¡çµ„)
  * **Stage 2**ï¼šç¾…é¦¬æ‹¼éŸ³ âœ å°èªæ¼¢å­— (hok2han æ¨¡çµ„)
* **åŸºæ–¼ LoRA å¾®èª¿çš„ Whisper æ¨¡å‹**ï¼š
  * **Mode 1**ï¼šå°èªèªéŸ³ âœ å°èªæ¼¢å­— (lora-whisper æ¨¡çµ„)
  * **Mode 2**ï¼šå°èªèªéŸ³ âœ ä¸­æ–‡æ–‡å­— (lora-whisper-zh æ¨¡çµ„)
* **å¤šæ¨¡å‹æ”¯æ´**ï¼šCTC-Based (Wav2Vec2)ã€Transformer Seq2Seqã€Whisper + LoRA å¾®èª¿
* **é«˜æ•ˆè¨“ç·´**ï¼šæ··åˆç²¾åº¦ (AMP)ã€LoRA åƒæ•¸é«˜æ•ˆå¾®èª¿ã€Accelerate åˆ†æ•£å¼è¨“ç·´
* **æ˜“ç”¨éƒ¨ç½²**ï¼šHugging Face Hub / Spaces ä¸€éµä¸Šå‚³ã€Dockerfile
* **é–‹æ”¾åŸå§‹ç¢¼**ï¼šApach 2.0ï¼Œæ­¡è¿å­¸è¡“èˆ‡éå•†æ¥­ç”¨é€”

---

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹

```
taiwanese-speech-to-text/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ è©æ¢éŸ³æª”/...
â”‚   â”œâ”€â”€ ä¾‹å¥éŸ³æª”/...
â”‚   â””â”€â”€ kautian.ods
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ my-wav2vec2/...
â”‚   â”œâ”€â”€ hok2han/...
â”‚   â”œâ”€â”€ lora-whisper/...
â”‚   â””â”€â”€ lora-whisper-zh/...
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ“¥ å®‰è£èˆ‡ç’°å¢ƒæº–å‚™

1. **Clone å°ˆæ¡ˆ**

   ```bash
   git clone https://github.com/KikKoh/taiwanese-speech-to-text.git
   cd taiwanese-speech-to-text
   ```
2. **å»ºç«‹è™›æ“¬ç’°å¢ƒ**ï¼ˆå»ºè­°ä½¿ç”¨ `venv` æˆ– `conda`ï¼‰

   ```bash
   python3.10 -m venv venv
   source venv/bin/activate
   ```
3. **å®‰è£ç›¸ä¾å¥—ä»¶**

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```
4. **(å¯é¸) å®‰è£ GPU / Accelerate æ”¯æ´**

   ```bash
   pip install accelerate
   accelerate config  # åˆå§‹åŒ–è¨­å®š
   ```

---

## ğŸš€ å¿«é€Ÿä¸Šæ‰‹

### 1. æ¨è«–ï¼šèªéŸ³ âœ ç¾…é¦¬æ‹¼éŸ³

```python
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import soundfile as sf

processor = Wav2Vec2Processor.from_pretrained("my-wav2vec2")
model = Wav2Vec2ForCTC.from_pretrained("my-wav2vec2").to(device)
model.eval()

waveform, sr = sf.read("audio.wav")
waveform = torch.tensor(waveform).float()
if waveform.dim() == 1:
  waveform = waveform.unsqueeze(0)
else:
  waveform = waveform.permute(1, 0)
if waveform.shape[0] > 1:
  waveform = waveform.mean(dim=0, keepdim=True)
if sr != target_sample_rate:
  resampler = torchaudio.transforms.Resample(sr, 16000)
  waveform = resampler(waveform)

audio_input = processor(waveform, sampling_rate=16000, return_tensors="pt")
with torch.no_grad():
    logits = model(audio_input.input_values).logits
pred_ids = torch.argmax(logits, dim=-1)[0].tolist()
romaji = processor.batch_decode(pred_ids)
print("ç¾…é¦¬æ‹¼éŸ³ï¼š", romaji)
```

### 2. æ¨è«–ï¼šæ‹¼éŸ³ âœ å°èªæ¼¢å­—

```python
from transformers import AutoTokenizer
from model.hok2han import Seq2SeqTransformer

input_tokenizer = AutoTokenizer.from_pretrained("KikKoh/Hok2Han", subfolder="input_tokenizer")
output_tokenizer = AutoTokenizer.from_pretrained("KikKoh/Hok2Han", subfolder="output_tokenizer")
model = Seq2SeqTransformer.from_pretrained("KikKoh/Hok2Han").to(device)
model.eval()

encoded = input_tokenizer("pinyin", max_length=max_len, padding="max_length", truncation=True, return_tensors="pt")
input_ids = encoded["input_ids"].to(device)
attention_mask = encoded["attention_mask"].to(device)

start_token_id = output_tokenizer.cls_token_id or output_tokenizer.convert_tokens_to_ids('<s>')
end_token_id = output_tokenizer.sep_token_id or output_tokenizer.convert_tokens_to_ids('</s>')

tgt_ids = torch.tensor([[start_token_id]], device=device)
total_confidence = 0.0
token_count = 0

for _ in range(max_len - 1):
  tgt_mask = generate_square_subsequent_mask(tgt_ids.size(1)).to(device)
  tgt_key_padding_mask = (tgt_ids == output_tokenizer.pad_token_id).to(device)

  outputs = model(input_ids, tgt_ids, input_tokenizer.pad_token_id, =output_tokenizer.pad_token_id,
            src_key_padding_mask=(attention_mask == 0), tgt_key_padding_mask=tgt_key_padding_mask)
  next_token_logits = outputs[:, -1, :]
  probs = softmax(next_token_logits, dim=-1)
  next_token = torch.argmax(probs, dim=-1).unsqueeze(1)
  tgt_ids = torch.cat([tgt_ids, next_token], dim=1)
  if next_token.item() == end_token_id:
    break
translation = output_tokenizer.decode(tgt_ids[0], skip_special_tokens=True).replace(" ", "")
print("å°èªæ¼¢å­—ï¼š", translation)
```

### 3. æ¨è«–ï¼šWhisper + LoRA(zh)

```python
from peft import PeftModel
from transformers import WhisperForConditionalGeneration, WhisperProcessor

processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="zh", task="transcribe")
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
model = PeftModel.from_pretrained(model, "demo/lora-whisper").to(device).eval()

waveform, sr = sf.read("audio.wav")
waveform = torch.tensor(waveform).float()
if waveform.dim() == 1:
  waveform = waveform.unsqueeze(0)
else:
  waveform = waveform.permute(1, 0)
if waveform.shape[0] > 1:
  waveform = waveform.mean(dim=0, keepdim=True)
if sr != target_sample_rate:
  resampler = torchaudio.transforms.Resample(sr, 16000)
  waveform = resampler(waveform)

inputs = processor(waveform, sampling_rate=sample_rate, return_tensors="pt", padding=True)
with torch.no_grad():
  generated_ids = model.generate(input_features=inputs.input_features.to(device), task="transcribe", language="zh")
transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print("ç”Ÿæˆæ–‡æœ¬ï¼š", transcription)
```

---

## ğŸ› ï¸ è‡ªè¨‚è¨“ç·´

å„å­æ¨¡çµ„å·²æä¾›å®Œæ•´ `README.md`ï¼Œç¯„ä¾‹è¨“ç·´æµç¨‹åŒ…å«ï¼š

* **my-wav2vec2**ï¼šCTC å¾®èª¿ã€AMPã€ç·šæ€§ warm-up scheduler
* **hok2han**ï¼šè‡ªæ¶ Seq2Seq Transformerã€CrossEntropyLossã€Early Stopping
* **lora-whisper / lora-whisper-zh**ï¼šLoRA å¾®èª¿ã€Accelerate åˆ†æ•£å¼è¨“ç·´ã€WER é©—è­‰

è«‹åƒè€ƒå°æ‡‰è³‡æ–™å¤¾ä¸‹çš„ `README.md`ï¼Œä¸¦ä¾æ“š GPU è¨ˆç®—è³‡æºã€èªæ–™è¦æ¨¡èª¿æ•´è¶…åƒæ•¸ã€‚

---

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿å°èªæ„›å¥½è€…ã€èªéŸ³è™•ç†ç ”ç©¶è€…ã€AI é–‹ç™¼è€…åƒèˆ‡ï¼š

1. Fork æœ¬å€‰åº«ä¸¦å»ºç«‹åˆ†æ”¯ (`git checkout -b feature/xxx`)
2. å®Œæˆé–‹ç™¼å¾Œæäº¤ PRï¼Œè©³è¿°ä¿®æ”¹å…§å®¹èˆ‡æ¸¬è©¦çµæœ
3. Issue ä¸­ææ¡ˆæˆ–è¨è«–æ–°åŠŸèƒ½

---

## ğŸ“œ æˆæ¬Šæ¢æ¬¾

* **ç¨‹å¼ç¢¼**ï¼šMIT License ([LICENSE](./LICENSE))
* **èªæ–™è³‡æ–™**ï¼šä¾æ“šä¸­è¯æ°‘åœ‹æ•™è‚²éƒ¨ã€Šè‡ºç£é–©å—èªå¸¸ç”¨è©è¾­å…¸ã€‹CC BY-ND 3.0 TW æ¢æ¬¾ï¼Œåƒ…ç”¨æ–¼å­¸è¡“ç ”ç©¶èˆ‡éå•†æ¥­ç”¨é€”

---

## âœ‰ï¸ è¯çµ¡æ–¹å¼

å¦‚æœ‰ç–‘å•ï¼Œè«‹é€é GitHub Issue æˆ–ç§è¨Šè¯çµ¡ï¼š

* GitHub: [@10809104](https://github.com/10809104)
* Hugging Face Spaces: [KikKoh/Hokkien](https://huggingface.co/spaces/KikKoh/Hokkien)
* facebook: [KikKoh2024](https://www.facebook.com/kikkoh.2024))

---

ç¥ç ”ç©¶é †åˆ©ï¼ŒæœŸå¾…æ‚¨çš„è²¢ç»ï¼
